{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import xlrd, xlwt\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "    total_words = []\n",
    "    tokenized = []\n",
    "    \n",
    "    for pattern in data:\n",
    "        sts = \"\"\n",
    "        for word in pattern:\n",
    "            if word not in pre_stopwords:\n",
    "                sts += word\n",
    "        t = jieba.lcut(sts)\n",
    "        t_s = []\n",
    "        for word in t:\n",
    "            if word not in stopwords:\n",
    "                t_s.append(word)\n",
    "        total_words.extend(t_s)\n",
    "        tokenized.append(t_s)\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "def build_word2vec(data, w2v_dims, w2v_model=None, update=True):\n",
    "    date = time.strftime(\"%Y%m%d\")\n",
    "    tokenized = []\n",
    "    max_len = 0\n",
    "    \n",
    "    for pattern in data:\n",
    "#         求全数据集中句子的最长长度\n",
    "        if len(pattern) > max_len:\n",
    "            max_len = len(pattern)\n",
    "        t = jieba.lcut(pattern)\n",
    "        t_s = []\n",
    "#         去后停用词\n",
    "        for word in t:\n",
    "            if word not in stopwords:\n",
    "                t_s.append(word)\n",
    "        tokenized.append(t_s)\n",
    "        \n",
    "# 若没有传入词向量模型，生成新的模型\n",
    "    if w2v_model is None:\n",
    "        print('正在生成新的词向量模型...')\n",
    "        w2v_model = Word2Vec(tokenized,sg=1,size=w2v_dims,window=5,min_count=1,negative=1,sample=0.001,hs=1)\n",
    "        w2v_model.train(tokenized, total_examples=len(tokenized), epochs=5)\n",
    "# 若传入了词向量模型，则更新模型\n",
    "    elif update is True:\n",
    "        print('正在更新词向量...')\n",
    "        w2v_model = update_w2v(w2v_model, tokenized)\n",
    "    return w2v_model, max_len\n",
    "\n",
    "# 计算短文本的平均词向量\n",
    "def avg_feature_vector(sentence, index2word_set):\n",
    "    words = jieba.lcut(sentence)\n",
    "#     print(words)\n",
    "    feature_vec = np.zeros((w2v_dims, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    i = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "#             print(model[word])\n",
    "#             print(tf_idf[i])\n",
    "            feature_vec = np.add(feature_vec, w2v_model[word])\n",
    "#             print(feature_vec)\n",
    "#             sleep(1)\n",
    "        i += 1\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n",
    "\n",
    "# 构建词向量模型\n",
    "def build_wd2idx_embedMatrix(w2vModel):\n",
    "    word2idx = {\"_stopWord\": 0}  # 这里加了一行是用来过滤停用词的。\n",
    "    vocab_list = [(w, w2vModel.wv[w]) for w, v in w2vModel.wv.vocab.items()]\n",
    "    embedMatrix = np.zeros((len(w2vModel.wv.vocab.items()) + 1, w2vModel.vector_size))\n",
    "\n",
    "    for i in range(0, len(vocab_list)):\n",
    "        word = vocab_list[i][0]\n",
    "        word2idx[word] = i + 1\n",
    "        embedMatrix[i + 1] = vocab_list[i][1]\n",
    "    return word2idx, embedMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在生成新的词向量模型...\n",
      "词向量模型保存成功：20190607_词向量_w2v_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:60: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# 训练集和对应标签预处理\n",
    "date = time.strftime(\"%Y%m%d\")\n",
    "readbook = xlrd.open_workbook('语音转文本_全业务数据集.xlsx')\n",
    "sheet1 = readbook.sheet_by_name('全集')\n",
    "sheet2 = readbook.sheet_by_name('清洗后')\n",
    "stopwords = [line.strip() for line in open('stopwords.txt',encoding='gb18030',errors='ignor').readlines()]\n",
    "# 加载自定义词典\n",
    "jieba.load_userdict(\"催收文本-newdic.txt\")\n",
    "\n",
    "global w2v_dims, w2v_model\n",
    "w2v_dims = 200\n",
    "\n",
    "# 读取全业务数据集\n",
    "total_data = []\n",
    "for i in range(sheet1.nrows-1):\n",
    "    total_data.append(sheet1.cell(i,0).value)\n",
    "\n",
    "# 读取清洗后的数据集\n",
    "data_set = []\n",
    "for i in range(sheet2.nrows-1):\n",
    "    data_set.append(sheet2.cell(i+1,0).value)\n",
    "\n",
    "# 创建词向量模型\n",
    "# w2v_model = Word2Vec.load('20190521_w2v_model')\n",
    "w2v_model, max_len = build_word2vec(total_data, w2v_dims, update=True)\n",
    "w2v_model.save(date+'_词向量_w2v_model')# 保存模型\n",
    "print('词向量模型保存成功：%s_词向量_w2v_model' %date)\n",
    "\n",
    "# index2word_set = set(w2v_model.index2word)\n",
    "wd2idx, embedMatrix = build_wd2idx_embedMatrix(w2v_model)\n",
    "\n",
    "dictionary = []\n",
    "Sts2Vec = []\n",
    "for i in data_set:\n",
    "    Vec = avg_feature_vector(i, wd2idx)\n",
    "    Sts2Vec.append(Vec)\n",
    "    dictionary.append({'sts': i, 'sts2vec': Vec})\n",
    "# sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "# print(Sts2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 200\n",
    "km_cluster = KMeans(n_clusters=num_clusters, max_iter=10000, n_init=1, \\\n",
    "                    init='k-means++',n_jobs=1)\n",
    "\n",
    "result = km_cluster.fit_predict(Sts2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = []\n",
    "count = np.zeros(num_clusters)\n",
    "for i in range(len(result)):\n",
    "    count[result[i]] += 1\n",
    "#     print(result[0])\n",
    "\n",
    "# print(count)\n",
    "\n",
    "for i in range(len(count)):\n",
    "    count_dict.append({'count': i, 'num': count[i]})\n",
    "# print(count_dict)\n",
    "\n",
    "for i in range(len(count_dict)):\n",
    "    for j in range(len(count_dict)-i-1):\n",
    "        if count_dict[j]['num'] > count_dict[j+1]['num']:\n",
    "            num_tmp = count_dict[j]['num']\n",
    "            count_tmp = count_dict[j]['count']\n",
    "            count_dict[j]['num'] = count_dict[j+1]['num']\n",
    "            count_dict[j]['count'] = count_dict[j+1]['count']\n",
    "            count_dict[j+1]['num'] = num_tmp\n",
    "            count_dict[j+1]['count'] = count_tmp\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 2.0\n",
      "7 7.0\n",
      "143 13.0\n",
      "195 13.0\n",
      "128 15.0\n",
      "52 18.0\n",
      "190 19.0\n",
      "45 22.0\n",
      "59 24.0\n",
      "161 30.0\n",
      "91 32.0\n",
      "174 32.0\n",
      "197 39.0\n",
      "141 44.0\n",
      "158 44.0\n",
      "155 46.0\n",
      "46 48.0\n",
      "184 50.0\n",
      "35 53.0\n",
      "169 53.0\n",
      "67 54.0\n",
      "83 54.0\n",
      "84 54.0\n",
      "76 57.0\n",
      "92 58.0\n",
      "129 58.0\n",
      "135 62.0\n",
      "170 63.0\n",
      "178 63.0\n",
      "116 65.0\n",
      "103 66.0\n",
      "199 67.0\n",
      "165 69.0\n",
      "0 71.0\n",
      "144 71.0\n",
      "163 71.0\n",
      "65 72.0\n",
      "185 72.0\n",
      "66 76.0\n",
      "154 76.0\n",
      "125 80.0\n",
      "146 81.0\n",
      "149 83.0\n",
      "147 85.0\n",
      "171 86.0\n",
      "160 90.0\n",
      "188 90.0\n",
      "114 93.0\n",
      "198 93.0\n",
      "38 94.0\n",
      "175 95.0\n",
      "130 101.0\n",
      "133 101.0\n",
      "14 103.0\n",
      "17 103.0\n",
      "94 103.0\n",
      "44 105.0\n",
      "3 106.0\n",
      "150 106.0\n",
      "12 108.0\n",
      "131 109.0\n",
      "97 112.0\n",
      "137 114.0\n",
      "168 114.0\n",
      "186 114.0\n",
      "27 115.0\n",
      "20 116.0\n",
      "51 120.0\n",
      "56 120.0\n",
      "50 121.0\n",
      "49 122.0\n",
      "85 127.0\n",
      "142 128.0\n",
      "30 129.0\n",
      "118 131.0\n",
      "68 134.0\n",
      "126 134.0\n",
      "138 135.0\n",
      "86 138.0\n",
      "111 146.0\n",
      "62 148.0\n",
      "134 152.0\n",
      "15 157.0\n",
      "113 158.0\n",
      "53 160.0\n",
      "93 164.0\n",
      "121 165.0\n",
      "69 166.0\n",
      "61 167.0\n",
      "151 168.0\n",
      "18 169.0\n",
      "179 169.0\n",
      "47 171.0\n",
      "115 171.0\n",
      "98 172.0\n",
      "122 182.0\n",
      "37 184.0\n",
      "152 184.0\n",
      "159 186.0\n",
      "70 187.0\n",
      "127 188.0\n",
      "89 189.0\n",
      "140 189.0\n",
      "107 190.0\n",
      "23 191.0\n",
      "73 191.0\n",
      "28 192.0\n",
      "132 193.0\n",
      "78 198.0\n",
      "77 199.0\n",
      "100 200.0\n",
      "101 201.0\n",
      "96 205.0\n",
      "110 207.0\n",
      "182 207.0\n",
      "157 211.0\n",
      "24 220.0\n",
      "81 220.0\n",
      "167 220.0\n",
      "123 231.0\n",
      "13 233.0\n",
      "36 233.0\n",
      "104 241.0\n",
      "58 244.0\n",
      "42 246.0\n",
      "194 247.0\n",
      "109 250.0\n",
      "11 251.0\n",
      "172 252.0\n",
      "39 254.0\n",
      "1 264.0\n",
      "25 264.0\n",
      "90 266.0\n",
      "148 268.0\n",
      "173 269.0\n",
      "16 275.0\n",
      "55 276.0\n",
      "136 276.0\n",
      "31 283.0\n",
      "80 310.0\n",
      "57 313.0\n",
      "191 325.0\n",
      "156 335.0\n",
      "108 349.0\n",
      "176 350.0\n",
      "75 354.0\n",
      "22 355.0\n",
      "54 357.0\n",
      "48 359.0\n",
      "5 361.0\n",
      "192 361.0\n",
      "4 363.0\n",
      "26 364.0\n",
      "193 368.0\n",
      "87 371.0\n",
      "187 374.0\n",
      "166 377.0\n",
      "21 378.0\n",
      "177 380.0\n",
      "6 396.0\n",
      "102 415.0\n",
      "180 430.0\n",
      "145 437.0\n",
      "183 438.0\n",
      "9 445.0\n",
      "162 451.0\n",
      "63 453.0\n",
      "43 458.0\n",
      "74 463.0\n",
      "29 470.0\n",
      "10 475.0\n",
      "79 478.0\n",
      "34 487.0\n",
      "72 490.0\n",
      "119 556.0\n",
      "8 557.0\n",
      "60 583.0\n",
      "64 601.0\n",
      "189 629.0\n",
      "33 635.0\n",
      "164 647.0\n",
      "88 658.0\n",
      "40 692.0\n",
      "106 718.0\n",
      "112 782.0\n",
      "2 810.0\n",
      "99 820.0\n",
      "196 826.0\n",
      "82 840.0\n",
      "41 848.0\n",
      "153 959.0\n",
      "117 983.0\n",
      "71 1029.0\n",
      "19 1124.0\n",
      "124 1126.0\n",
      "32 1158.0\n",
      "95 1162.0\n",
      "139 1186.0\n",
      "105 1341.0\n",
      "181 1370.0\n"
     ]
    }
   ],
   "source": [
    "for i in count_dict:\n",
    "    print(i['count'], i['num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "9\n",
      "14\n",
      "15\n",
      "27\n",
      "30\n",
      "37\n",
      "53\n",
      "57\n",
      "69\n",
      "73\n",
      "86\n",
      "87\n",
      "90\n",
      "92\n",
      "94\n",
      "100\n",
      "101\n",
      "102\n",
      "108\n",
      "113\n",
      "118\n",
      "127\n",
      "130\n",
      "132\n",
      "134\n",
      "137\n",
      "140\n",
      "148\n",
      "156\n",
      "166\n",
      "168\n",
      "183\n",
      "187\n",
      "189\n",
      "197\n",
      "198\n",
      "203\n",
      "212\n",
      "214\n",
      "215\n",
      "217\n",
      "224\n",
      "227\n",
      "229\n",
      "230\n",
      "232\n",
      "236\n",
      "239\n",
      "243\n",
      "244\n",
      "247\n",
      "248\n",
      "249\n",
      "252\n",
      "254\n",
      "260\n",
      "261\n",
      "264\n",
      "270\n",
      "271\n",
      "279\n",
      "284\n",
      "287\n",
      "288\n",
      "292\n",
      "295\n",
      "300\n",
      "301\n",
      "303\n",
      "305\n",
      "308\n",
      "313\n",
      "314\n",
      "316\n",
      "322\n",
      "323\n",
      "326\n",
      "328\n",
      "330\n",
      "334\n",
      "337\n",
      "338\n",
      "339\n",
      "342\n",
      "343\n",
      "344\n",
      "348\n",
      "349\n",
      "352\n",
      "359\n",
      "361\n",
      "366\n",
      "368\n",
      "374\n",
      "378\n",
      "379\n",
      "388\n",
      "392\n",
      "396\n",
      "398\n",
      "400\n",
      "403\n",
      "405\n",
      "406\n",
      "407\n",
      "415\n",
      "422\n",
      "424\n",
      "425\n",
      "429\n",
      "431\n",
      "434\n",
      "435\n",
      "440\n",
      "441\n",
      "442\n",
      "446\n",
      "453\n",
      "455\n",
      "456\n",
      "458\n",
      "463\n",
      "464\n",
      "465\n",
      "468\n",
      "473\n",
      "474\n",
      "476\n",
      "485\n",
      "486\n",
      "488\n",
      "492\n",
      "498\n",
      "501\n",
      "502\n",
      "505\n",
      "506\n",
      "511\n",
      "512\n",
      "514\n",
      "518\n",
      "520\n",
      "524\n",
      "529\n",
      "530\n",
      "531\n",
      "534\n",
      "536\n",
      "537\n",
      "538\n",
      "540\n",
      "544\n",
      "545\n",
      "546\n",
      "550\n",
      "554\n",
      "557\n",
      "559\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "574\n",
      "576\n",
      "577\n",
      "578\n",
      "581\n",
      "584\n",
      "586\n",
      "588\n",
      "591\n",
      "598\n",
      "600\n",
      "601\n",
      "604\n",
      "605\n",
      "615\n",
      "622\n",
      "623\n",
      "628\n",
      "630\n",
      "633\n",
      "641\n",
      "649\n",
      "650\n",
      "653\n",
      "655\n",
      "656\n",
      "657\n",
      "659\n",
      "661\n",
      "664\n",
      "667\n",
      "670\n",
      "672\n",
      "675\n",
      "678\n",
      "685\n",
      "686\n",
      "694\n",
      "696\n",
      "704\n",
      "706\n",
      "708\n",
      "709\n",
      "711\n",
      "714\n",
      "715\n",
      "716\n",
      "722\n",
      "723\n",
      "730\n",
      "733\n",
      "736\n",
      "740\n",
      "747\n",
      "748\n",
      "751\n",
      "752\n",
      "757\n",
      "759\n",
      "762\n",
      "767\n",
      "768\n",
      "769\n",
      "775\n",
      "789\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "799\n",
      "811\n",
      "812\n",
      "814\n",
      "816\n",
      "818\n",
      "819\n",
      "822\n",
      "824\n",
      "825\n",
      "828\n",
      "831\n",
      "833\n",
      "835\n",
      "836\n",
      "839\n",
      "840\n",
      "841\n",
      "846\n",
      "847\n",
      "850\n",
      "853\n",
      "855\n",
      "856\n",
      "859\n",
      "860\n",
      "861\n",
      "864\n",
      "866\n",
      "871\n",
      "875\n",
      "877\n",
      "878\n",
      "880\n",
      "883\n",
      "886\n",
      "887\n",
      "890\n",
      "893\n",
      "894\n",
      "899\n",
      "904\n",
      "905\n",
      "908\n",
      "910\n",
      "913\n",
      "916\n",
      "917\n",
      "918\n",
      "922\n",
      "923\n",
      "926\n",
      "927\n",
      "930\n",
      "935\n",
      "936\n",
      "937\n",
      "939\n",
      "942\n",
      "948\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "965\n",
      "968\n",
      "969\n",
      "970\n",
      "972\n",
      "977\n",
      "979\n",
      "982\n",
      "983\n",
      "993\n",
      "994\n",
      "995\n",
      "997\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(count)):\n",
    "    if count[i]<10:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句向量聚类结果已保存：20190607句向量-聚类.txt\n"
     ]
    }
   ],
   "source": [
    "fobj = open(date+'句向量-聚类.txt','a')\n",
    "for i in result:\n",
    "    fobj.write('\\n'+str(i))\n",
    "print('句向量聚类结果已保存：'+date+'句向量-聚类.txt')\n",
    "fobj.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
