{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# 先进行本地/全局分类，再进行类目分类\n",
    "# 两次分类，现将用户输入按照意愿进行分类（愿意还款，意图模糊，延期还款，其它），再根据不同意愿进行进一步分类\n",
    "# 理论上，意愿分类的准确率会很高，限定意愿后的进一步分类准确率也会有所提高\n",
    "# 仅使用多卷积核CNN进行特征的综合提取\n",
    "# 对于池化尺寸的大小未定，需加大样本数量进行对比\n",
    "import keras\n",
    "import xlrd, xlwt\n",
    "from xlutils import copy\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras import regularizers\n",
    "from keras.layers.embeddings import Embedding\n",
    "import jieba\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout, LSTM, SpatialDropout1D, LeakyReLU, Input, concatenate\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "对用户意图进行多次分：全局/局部——粗分类——细分类\n",
    "可以有效减少多分类情境下模型难收敛的问题\n",
    "逐层逼近用户的真实意图，越底层容错率越高\n",
    "'''\n",
    "# jieba分词+去停用词\n",
    "def tokenize(data):\n",
    "    tokenized = []\n",
    "#     对list类型数据进行分词 \n",
    "    if isinstance(data, list):\n",
    "        for pattern in data:\n",
    "            t = jieba.lcut(pattern)\n",
    "            t_s = []\n",
    "#             去停用词\n",
    "            for word in t:\n",
    "                if word not in stopwords:\n",
    "                    t_s.append(word)\n",
    "            tokenized.append(t_s)\n",
    "#     对str类型数据进行分词 \n",
    "    elif isinstance(data, str):\n",
    "        t = jieba.lcut(data)\n",
    "        t_s = []\n",
    "#             去停用词\n",
    "        for word in t:\n",
    "            if word not in stopwords:\n",
    "                t_s.append(word)\n",
    "        tokenized.append(t_s)\n",
    "    return tokenized\n",
    "\n",
    "# 对神经网络模型、数据及标签进行预处理\n",
    "def preprocessing(data_set, label, num_classes, seed, mode):\n",
    "    tokenized = tokenize(data_set)\n",
    "    identify = wd_encode(wd2idx, tokenized)\n",
    "    padded_data,max_len = padding_sts(identify)\n",
    "    label = label_preprocessing(label, num_classes)\n",
    "# 将映射关系构造成字典\n",
    "    dictionary = []\n",
    "    for i in range(len(tokenized)):\n",
    "        dictionary.append({'sentence':data_set[i-1], 'tokenized':tokenized[i-1],\\\n",
    "                           'id':padded_data[i-1], 'label':label[i-1]})\n",
    "    # 对数据集进行切分，生成训练集和验证集\n",
    "    train_data, validate_data, train_label, validate_label = \\\n",
    "    split_data_set(padded_data, label, 0.1, detal=True)\n",
    "    if mode == 0:\n",
    "        # mode=0: 预测模式，需要再将训练集切分为训练集和测试集\n",
    "        train_data, test_data, train_label, test_label = \\\n",
    "        split_data_set(train_data, train_label, 0.222, detal=True)\n",
    "#     预测模式下不需要测试集\n",
    "    else:\n",
    "        test_data = None\n",
    "        test_label = None\n",
    "#     构造神经网络模型\n",
    "    model = build_model(max_len, num_classes)\n",
    "    \n",
    "    return dictionary, model, train_data, validate_data, test_data, train_label, validate_label, test_label\n",
    "\n",
    "# 将标签转换为指定的形式\n",
    "def label_preprocessing(label, num_classes):\n",
    "#     0: 愿意还款; 1: 意图模糊; 2: 延期还款; 3: 其他（全局）; \n",
    "    label = np.array(label)\n",
    "    categorical_labels = to_categorical(label, num_classes=num_classes)\n",
    "    return categorical_labels\n",
    "\n",
    "# 在预测时，寻找包含新词的句子\n",
    "def find_new_wd_of_sts(data, new_wd_of_sts_idx):\n",
    "    fobj = open(pathDir+'包含新词的句子.txt','a')\n",
    "    for i in new_wd_of_sts_idx:\n",
    "        fobj.write('\\n'+data[i])\n",
    "    print('save the new word of setense')\n",
    "    fobj.close()\n",
    "\n",
    "# 短句补零，使之与神经网络模型的embedding层的输入尺寸相同\n",
    "def padding_sts(identify,max_len=None):\n",
    "#     若不预设最大长度，则计算传入样本中的最大长度\n",
    "    if max_len is None:\n",
    "        max_len = 0\n",
    "        for sts in identify:\n",
    "            if len(sts)>max_len:\n",
    "                max_len = len(sts)\n",
    "    padded_id = list(map(lambda l:l + [0]*(max_len - len(l)), identify))\n",
    "    padded_id = np.array(padded_id)\n",
    "    return padded_id, max_len\n",
    "\n",
    "# 数据集切分，默认不显示detail\n",
    "def split_data_set(data, label, ratio, detal=False):\n",
    "    data_1, data_2, label_1, label_2 = train_test_split(data, label, test_size=ratio, random_state=seed)\n",
    "# data：待划分的样本特征集\n",
    "# label：待划分的样本标签\n",
    "# ratio：划分比例。如果是浮点数，在0-1之间，表示样本占比；如果是整数，表示样本数量\n",
    "# seed：是随机数的种子。\n",
    "# detal：显示分割后的详情。默认False\n",
    "    if detal == True:\n",
    "        print(\"data_1_len: \", len(data_1),\"label_1_len: \", len(label_1),\\\n",
    "              \"\\ndata_2_len: \", len(data_2),\"lebal_2_len: \", len(label_2))\n",
    "    return data_1, data_2, label_1, label_2\n",
    "\n",
    "# 预训练词向量模型\n",
    "def build_word2vec(filename, w2v_model=None, update=True):\n",
    "#     若传入了词向量模型，则直接载入模型，默认不更新词向量模型（更新策略效果暂时不明显，待调试）\n",
    "    if  w2v_model != None:\n",
    "        print('Loading Word2Vec model...')\n",
    "        try:\n",
    "            w2v_model = Word2Vec.load(pathDir+w2v_model)\n",
    "        except Exception as e:\n",
    "            print('Warning! Fail to load: %s.\\nBegin to build new w2v_model...' %e)\n",
    "            w2v_model = None\n",
    "#     如果需要构建新的词向量，则无需更新词向量\n",
    "    if w2v_model is None:\n",
    "        update = False\n",
    "#     如果需要更新词向量，则对词向量进行更新\n",
    "    if update is True:\n",
    "        try:\n",
    "            print('Update Word2Vec model...')        \n",
    "            w2v_model = update_w2v(w2v_model, tokenized)\n",
    "        except Exception as e:\n",
    "            print('Warning! Fail to update: %s' %e)\n",
    "        return w2v_model\n",
    "#     若需要更新词向量或重新构建词向量，则需获取样本数据集\n",
    "    else:\n",
    "#         获取全数据集样本（用做词向量训练）\n",
    "        try:\n",
    "            total_set = xlrd.open_workbook(filename)\n",
    "            total_set_sheet1 = total_set.sheet_by_index(0)\n",
    "        except Exception as e:\n",
    "            print('Error! Fail to load data_set: %s' %e)\n",
    "            sys.exit()\n",
    "        total_data = []\n",
    "        for i in range(total_set_sheet1.nrows-1):\n",
    "            total_data.append(total_set_sheet1.cell(i+1,0).value)\n",
    "#         对数据集进行分词\n",
    "        tokenized = tokenize(total_data)\n",
    "#         若没有传入词向量模型，构建新的模型\n",
    "#         判断是否有预设w2v_dims，若无则default=200\n",
    "        try:\n",
    "            w2v_dims\n",
    "        except NameError:\n",
    "            w2v_dims = 200\n",
    "#         训练词向量模型\n",
    "        w2v_model = Word2Vec(tokenized,sg=1,size=w2v_dims,window=5,min_count=1,negative=1,sample=0.001,hs=1)\n",
    "        w2v_model.train(tokenized, total_examples=len(tokenized), epochs=5)\n",
    "        return w2v_model\n",
    "\n",
    "# 更新词向量模型（仅会对出现新词了的文本进行更新）\n",
    "def update_w2v(w2v_model, tokenized):\n",
    "#     生成原词向量的词-向量映射关系\n",
    "    vocab_list = []\n",
    "    for w, _ in w2v_model.wv.vocab.items():\n",
    "        vocab_list.append(w)\n",
    "#     寻找词向量模型中没有的新词\n",
    "    new_sts = []\n",
    "    new_wd = []\n",
    "    for sts in tokenized:\n",
    "        for wd in sts:\n",
    "            if wd not in vocab_list:\n",
    "                new_wd.append(wd)\n",
    "                new_sts.append(sts)\n",
    "                break\n",
    "    if new_sts != []:\n",
    "        print('发现新词：', new_wd, '\\n已对词向量模型进行更新！')\n",
    "        w2v_model.build_vocab(new_sts, update=True)\n",
    "#         w2v_model.train(new_sts,total_examples=w2v_model.corpus_count,epochs=1)\n",
    "    else:\n",
    "        print('未发现新词，没有更新模型！')\n",
    "    return w2v_model\n",
    "\n",
    "# 对数据集中的每个词，按照词-向量的索引进行编码，若出现了生词，则填0\n",
    "def wd_encode(wd2idx, tokenized):\n",
    "    identify = []\n",
    "    for sts in tokenized:\n",
    "        id_sts = []\n",
    "        for wd in sts:\n",
    "            try:\n",
    "                id_sts.append(wd2idx[wd])\n",
    "            except:\n",
    "                print('“'+wd+'”不在词向量模型中')\n",
    "                id_sts.append('0')\n",
    "        identify.append(id_sts)\n",
    "    return identify\n",
    "\n",
    "# 构建词向量的单词索引和embedding层矩阵\n",
    "def build_wd2idx_embedMatrix(w2vModel):\n",
    "    word2idx = {\"_stopWord\": 0}  # 停用词，生词和padding填充的0。\n",
    "    vocab_list = [(w, w2vModel.wv[w]) for w, v in w2vModel.wv.vocab.items()]\n",
    "    embedMatrix = np.zeros((len(w2vModel.wv.vocab.items()) + 1, w2vModel.vector_size))\n",
    "\n",
    "    for i in range(0, len(vocab_list)):\n",
    "        word = vocab_list[i][0]\n",
    "        word2idx[word] = i + 1\n",
    "        embedMatrix[i + 1] = vocab_list[i][1]\n",
    "    return word2idx, embedMatrix\n",
    "\n",
    "# 将深度学习过程中训练的词向量矩阵更新到词向量模型中\n",
    "def embed2w2v(w2vModel, embedMatrix):\n",
    "    print('正在根据学习过程中训练的词向量矩阵对词向量模型进行更新...')\n",
    "    i = 0\n",
    "    for w,_ in w2vModel.wv.vocab.items():\n",
    "        w2vModel.wv[w] = embedMatrix[i]\n",
    "        i += 1\n",
    "    print('词向量模型更新完成')\n",
    "    return w2vModel\n",
    "\n",
    "# 设计网络模型：LSTM+六个卷积核(各10个特征)+LeakyReLU激活函数\n",
    "def build_model(max_len, num_classes):\n",
    "    # Input\n",
    "    comment_seq = Input(shape=[max_len], name='x_seq')\n",
    "    # Embedding\n",
    "    emb_comment = Embedding(len(embedMatrix), len(embedMatrix[0]), weights=[embedMatrix],\\\n",
    "                            input_length=max_len, trainable=False)(comment_seq)\n",
    "    # LSTM\n",
    "    LSTM_1 = LSTM(units=32, dropout=0.2, recurrent_dropout=0.1, return_sequences=True)(emb_comment)\n",
    "    # LSTM_1 = LSTM(200, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)(emb_comment)\n",
    "    # model.add(LSTM(200, return_sequences=True))\n",
    "    # LSTM_2 = LSTM(200, return_sequences=False)(LSTM_1)\n",
    "    # conv\n",
    "    convs = []\n",
    "    kernel_size = [1,2,3,4,5,max_len]\n",
    "    for ksz in kernel_size:\n",
    "        l_conv = Conv1D(filters=10, kernel_size=ksz, strides=1, padding='valid',\\\n",
    "                       use_bias=True,kernel_initializer='glorot_uniform',\\\n",
    "                       bias_initializer='zeros',kernel_regularizer=regularizers.l2(0.0001))(LSTM_1)\n",
    "        l_conv = LeakyReLU(alpha=0.01)(l_conv)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "    merge = concatenate(convs, axis=1)\n",
    "    dropout = Dropout(0.3)(merge)\n",
    "    # output = Dense(120)(merge)\n",
    "    # output = LeakyReLU(alpha=0.05)(output)\n",
    "    output = Dense(num_classes,activation='softmax')(dropout)\n",
    "    model = Model([comment_seq], output)\n",
    "\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    Adam = keras.optimizers.Adam(lr=0.01)\n",
    "    model.compile(optimizer=Adam, loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "    ThisTime()\n",
    "    return model\n",
    "\n",
    "def train_model(train_data, validate_data, train_label, validate_label, model, name):\n",
    "    # 训练网络模型\n",
    "    # verbose:显示日志。verbose=0: 为不在标准输出流输出日志信息 verbose=1: 为输出进度条记录 verbose=2: 为每个epoch输出一行记录\n",
    "    start_time = time.time()\n",
    "    loss_tmp = 10\n",
    "    lr_pre = 0.01\n",
    "    lr = lr_pre\n",
    "    count = 1\n",
    "    threshold = 5\n",
    "#     动态学习速率衰减\n",
    "    while lr >= lr_pre/128:\n",
    "        hist = model.fit(train_data, train_label, epochs=1, verbose=1)\n",
    "#         每次epoch后使用验证集进行验证，防止对训练集的过拟合\n",
    "        loss, _ = model.evaluate(validate_data, validate_label, verbose=0)\n",
    "        print(loss)\n",
    "\n",
    "        if loss < loss_tmp:\n",
    "            count = 1\n",
    "            loss_tmp = loss\n",
    "            model.save(NN_model_path+name)\n",
    "        elif count >= threshold:\n",
    "            model = load_model(NN_model_path+name)\n",
    "            print(\"decay the learning rate\")\n",
    "            lr = lr/2\n",
    "            Adam = keras.optimizers.Adam(lr=lr)\n",
    "            model.compile(optimizer=Adam, loss='categorical_crossentropy', metrics=['acc'])\n",
    "            count = 1\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print (\"processing time:\", elapsed_time, \"seconds\")\n",
    "    return model\n",
    "\n",
    "def test_model(model, dictionary, test_data, test_label, detail=False):\n",
    "    # 显示识别错误的数据，并计算准确率\n",
    "    count = 0\n",
    "    sum_loss = 0\n",
    "    for i in range(len(test_data)):\n",
    "        loss, accuracy = model.evaluate(np.column_stack(test_data[i-1]), np.column_stack(test_label[i-1]), batch_size=1, verbose=0)\n",
    "        if accuracy == 0:\n",
    "            for j in dictionary:\n",
    "                if (j['id'] == test_data[i-1]).all():\n",
    "                    tmp = j['label'].tolist()\n",
    "                    print(j['sentence'], j['tokenized'], 'label: ', tmp.index(max(tmp)))\n",
    "        else:\n",
    "            count += 1\n",
    "        sum_loss += loss\n",
    "        loss = sum_loss/len(test_data)\n",
    "    accuracy = count/len(test_data)\n",
    "    if detail == True:\n",
    "        print('Accuracy: %f' % (accuracy*100))\n",
    "        print('Loss: %f' % (loss))\n",
    "        ThisTime()\n",
    "        \n",
    "# 通过迭代遍历树状结构的节点\n",
    "def find_son(count_lavel, label_index=None):\n",
    "    tmp = []\n",
    "    data = []\n",
    "    label = []\n",
    "#     第0层：为方便迭代而构造出的虚拟结点\n",
    "    if count_lavel == 0:\n",
    "        label_index = [0]# 初始化label_index\n",
    "    else:\n",
    "        node = label_index[-1]# 如果不是第0层，则当前节点为label_index的最后一个元素\n",
    "    for i in range(sheet1.nrows-1):\n",
    "#         若层级为第0层，直接读取数据第一层的和标签\n",
    "        if count_lavel == 0:\n",
    "            data.append(sheet1.cell(i+1, 0).value)\n",
    "            label.append(sheet1.cell(i+1, 1).value)\n",
    "        elif node == sheet1.cell(i+1, count_lavel).value:\n",
    "            for j in range(len(label_index)-1):\n",
    "                if sheet1.cell(i+1, j+1).value == label_index[j+1]:\n",
    "                    flag = True\n",
    "                else:\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag is True:\n",
    "                data.append(sheet1.cell(i+1, 0).value)\n",
    "                label.append(sheet1.cell(i+1, count_lavel+1).value)\n",
    "#     print(label)\n",
    "    tmp = list(set(label))# 获取该节点下的子节点\n",
    "#     如果有数据未标记，则抛出错误\n",
    "    if '' in tmp:\n",
    "        raise TypeError('The label in %d layer of some data is BLANK, which is illegal!(有数据未进行标记！)' %count_lavel+1)\n",
    "    if count_lavel == 0:\n",
    "        print('根节点下的子节点:', tmp)# 根节点没有node\n",
    "    else:    \n",
    "        print('第%d层级%d节点下的子节点:' %(count_lavel, node), tmp)\n",
    "#     当该节点下有多个子节点时，训练该节点\n",
    "    if len(tmp) > 1:\n",
    "        print('正在训练该节点模型...')\n",
    "#         构建模型\n",
    "        (dictionary, model, train_data, \\\n",
    "         validate_data, test_data, \\\n",
    "         train_label, validate_label, testllabel) = preprocessing(data, label, len(tmp), seed=seed, mode=devide_mode)\n",
    "#         模型命名\n",
    "        if count_lavel == 0:# 根节点没有node\n",
    "            model_name = '第%d层级的模型' %count_lavel\n",
    "        else:\n",
    "            model_name = '第%d层级第%d节点的模型' %(count_lavel, node)\n",
    "#         训练模型\n",
    "        model = train_model(train_data, validate_data, \\\n",
    "                            train_label, validate_label, model, model_name)\n",
    "        print('训练完成！模型名称: %s' %model_name)\n",
    "#         if count_lavel < num_label_lavel-1:# 循环遍历至倒数第二层（最后一层不存在子节点）\n",
    "#             count_lavel += 1\n",
    "#             for node in tmp:\n",
    "#                 print('遍历第%d层级%d节点下的子节点...' %(count_lavel, node))\n",
    "#                 print('_'*100)\n",
    "#                 label_index_tmp = label_index.copy()\n",
    "#                 label_index_tmp.append(node)\n",
    "#                 find_son(count_lavel, label_index_tmp)\n",
    "#             count_lavel -= 1# 循环遍历完本层的节点后，返回上一层\n",
    "#             print('返回至第%d层' %count_lavel)\n",
    "    else:\n",
    "        print('该节点无需训练模型')\n",
    "#     递归至倒数第二层（最后一层不可能存在子节点）\n",
    "    if count_lavel < num_label_lavel-1:\n",
    "        count_lavel += 1\n",
    "#         获得该节点的子节点的数量\n",
    "        for node in tmp:\n",
    "            print('遍历第%d层级%d节点下的子节点...' %(count_lavel, node))\n",
    "            print('_'*100)\n",
    "            label_index_tmp = label_index.copy()# 缓存前层的节点索引\n",
    "            label_index_tmp.append(node)\n",
    "            find_son(count_lavel, label_index_tmp)\n",
    "        count_lavel -= 1# 循环遍历完本层的节点后，返回上一层\n",
    "        print('返回至第%d层' %count_lavel)    \n",
    "        \n",
    "def predict_data(count_lavel, data, threshold, last_label=None):\n",
    "#     如果该节点为根节点，使用单独的名称\n",
    "#     仅在第一次迭代（根节点处）进行分词\n",
    "    if count_lavel == 0:\n",
    "        model_name =  '第0层级的模型'\n",
    "        tok = True\n",
    "    else:\n",
    "        model_name = '第%d层级第%d节点的模型' %(count_lavel, last_label)\n",
    "        tok = False\n",
    "    #     分词\n",
    "    if tok is True:\n",
    "#         单句预测时，仅对单句进行去停用词+分词即可\n",
    "        tokenized = tokenize(data)\n",
    "    else:\n",
    "        tokenized = data\n",
    "#     查找model_name是否存在于已加载的模型中（此处需要模型按照规范命名，加载错模型会导致predict的报错）\n",
    "    for i in range(len(model_name_lst)):\n",
    "        if model_name == model_name_lst[i]['name']:\n",
    "            model = model_name_lst[i]['model']\n",
    "            break\n",
    "        else:\n",
    "            model = None\n",
    "    if model is None:\n",
    "#         print(count_lavel, model_name)\n",
    "        if count_lavel < num_label_lavel-1:\n",
    "            count_lavel += 1\n",
    "            output = predict_data(count_lavel, tokenized, threshold, last_label=0)\n",
    "        else:\n",
    "            output = 'Wrong!'\n",
    "        return output\n",
    "    \n",
    "    max_len = model.layers[1].output_shape[1]# 获取模型文本最大长度\n",
    "    num_classes = model.layers[23].output_shape[1]# 获取模型分类数量\n",
    "\n",
    "#     根据词向量模型映射词标签\n",
    "    identify = wd_encode(wd2idx, tokenized)\n",
    "#     限定输入文本的长度（不可超过模型最大长度）\n",
    "    if len(identify[0]) > max_len:\n",
    "        print('输入过长(超出%d)，仅截取前一部分' %max_len)\n",
    "        identify[0] = identify[0][0:max_len]\n",
    "    padded_data, _ = padding_sts(identify, max_len=max_len)\n",
    "\n",
    "    accuracy = model.predict(padded_data, batch_size=1, verbose=0, steps=None).tolist()\n",
    "#     print(accuracy)\n",
    "#     准确率未超过阈值则判定为nomatch\n",
    "    if max(accuracy[0]) >= threshold:\n",
    "        label = accuracy[0].index(max(accuracy[0]))\n",
    "#         print(label)\n",
    "        output = label\n",
    "#             当迭代至最后一层，不再进行迭代\n",
    "        if count_lavel < num_label_lavel-1:\n",
    "            count_lavel += 1\n",
    "            predict_data(count_lavel, tokenized, \\\n",
    "                         threshold=predict_threshold, last_label=label)\n",
    "    else:\n",
    "#         print('nomatch\\n')\n",
    "#         fobj.write('\\t'+'nomatch')\n",
    "        output = 'nomatch'\n",
    "    return output\n",
    "\n",
    "def mkdir(path):\n",
    "#     去除首位空格\n",
    "    path=path.strip()\n",
    "#     去除尾部 \\ 符号\n",
    "    path=path.rstrip(\"\\\\\")\n",
    "#     判断路径是否存在\n",
    "    isExists=os.path.exists(path)\n",
    "#     如果不存在则创建目录\n",
    "    if not isExists:\n",
    "        os.makedirs(path)  \n",
    "#         print(path+' Success to build the path!')\n",
    "        return True\n",
    "#     如果目录存在则不创建，并提示目录已存在\n",
    "    else:\n",
    "#         print(path+' The Dir is exsist!')\n",
    "        return False\n",
    "\n",
    "# 载入数据集和对应的多级标签\n",
    "def load_data_set(fname, pathDir=sys.path[0]+'\\\\'):\n",
    "    try:\n",
    "        readbook = xlrd.open_workbook(pathDir+fname)\n",
    "    except Exception as e:\n",
    "        raise TypeError('Erroe! Fail to load train_data: %s' %e)\n",
    "    sheet1 = readbook.sheet_by_index(0)# sheet1: 文本+标签\n",
    "    num_label_lavel = sheet1.ncols-1# 数据集构成为：一列为文本，其余为层级标签。\n",
    "    data = []\n",
    "    label = []\n",
    "    for i in range(sheet1.nrows-1):\n",
    "        data.append(sheet1.cell(i+1, 0).value)\n",
    "        lb_tmp = []\n",
    "        for layer in range(num_label_lavel):\n",
    "            lb_tmp.append(sheet1.cell(i+1, layer+1).value)\n",
    "        label.append(lb_tmp)\n",
    "    return data, label\n",
    "\n",
    "# 显示每次运行片段的时间\n",
    "def ThisTime():\n",
    "    print('This time is: ', time.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(count_lavel, data, label, last_label=None):\n",
    "#     如果该节点为根节点，使用单独的名称\n",
    "#     仅在第一次迭代（根节点处）对数据进行预处理（分词、去停用词等）\n",
    "    if count_lavel == 0:\n",
    "        model_name =  '第0层级的模型'\n",
    "        regu = True\n",
    "    else:\n",
    "        model_name = '第%d层级第%d节点的模型' %(count_lavel, last_label)\n",
    "        regu = False\n",
    "#     数据规范化\n",
    "    if regu is True:\n",
    "        tokenized = tokenize(data)# 分词+去停用词\n",
    "        identify = wd_encode(wd2idx, tokenized)# 根据词向量模型映射词标签\n",
    "    else:\n",
    "        identify = data\n",
    "#     查找model_name是否存在于已加载的模型中（此处需要模型按照规范命名，加载错模型会导致predict的报错）\n",
    "    for i in range(len(model_name_lst)):\n",
    "        if model_name == model_name_lst[i]['name']:\n",
    "            model = model_name_lst[i]['model']\n",
    "            break\n",
    "        else:\n",
    "            model = None\n",
    "#     如果当前节点没有模型，但不是倒数第二层，则继续向下递归\n",
    "    if model is None:\n",
    "#         print(count_lavel, model_name)\n",
    "        if count_lavel < num_label_lavel-1:\n",
    "            count_lavel += 1\n",
    "            output = test_data(count_lavel, identify, label, last_label=0)\n",
    "        else:\n",
    "            output = 'Wrong!'\n",
    "        return output\n",
    "#     如果当前节点有模型，则evaluate该节点\n",
    "    else:\n",
    "        max_len = model.layers[1].output_shape[1]# 获取模型文本最大长度\n",
    "        num_classes = model.layers[23].output_shape[1]# 获取模型分类数量(树的深度)\n",
    "\n",
    "# #         根据词向量模型映射词标签\n",
    "#         identify = wd_encode(wd2idx, tokenized)\n",
    "#         限定输入文本的长度（不可超过模型最大长度）\n",
    "        if len(identify[0]) > max_len:\n",
    "            print('输入过长(超出%d)，仅截取前一部分' %max_len)\n",
    "            identify[0] = identify[0][0:max_len]\n",
    "        padded_data, _ = padding_sts(identify, max_len=max_len)\n",
    "\n",
    "        loss, accuracy = model.evaluate(padded_data, np.column_stack(label[count_lavel+1]), batch_size=1, verbose=0)\n",
    "#         如果预测错误，则打印该句并返回False\n",
    "        if accuracy == 0:\n",
    "            for j in dictionary:\n",
    "                if (j['id'] == test_data[i-1]).all():\n",
    "#                         tmp = j['label'].tolist()\n",
    "                    print(j['sentence'], j['tokenized'], 'label: ', j['label'])\n",
    "                    return count, loss, False\n",
    "#         如果预测正确，则继续递归\n",
    "        elif count_lavel < num_label_lavel-1:\n",
    "            count_lavel += 1\n",
    "            count, loss, status = test_data(count_lavel, identify, label, last_label=label[count_lavel])\n",
    "        else:\n",
    "            count -= 1\n",
    "            return count, loss, True\n",
    "\n",
    "        if count_lavel == 0:\n",
    "            count += 1\n",
    "        count_lavel -= 1\n",
    "        return count, loss, status\n",
    "\n",
    "#             loss = sum_loss/len(test_data)\n",
    "#         accuracy = count/len(test_data)\n",
    "#         if detail is True:\n",
    "#             print('Accuracy: %f' % (accuracy*100))\n",
    "#             print('Loss: %f' % (loss))\n",
    "#             ThisTime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading self-defined dict...\n",
      "Loading stopwords...\n",
      "Loading Word2Vec model...\n",
      "词向量模型保存成功：w2v_model\n"
     ]
    }
   ],
   "source": [
    "# 定义全局变量\n",
    "global date, wd2idx, embedMatrix, max_len, seed,\\\n",
    "        w2v_dims, predict_threshold, model_name_lst, \\\n",
    "        devide_mode, num_label_lavel\n",
    "date = time.strftime(\"%Y%m%d\")# 用于保存和加载当天的模型\n",
    "seed = random.randint(1,10000)# 随机生成种子，确保每次切分得到的数据集不同\n",
    "max_len = 50# 初始化单句最大词数。超出最大长度则丢弃，不足则填0\n",
    "devide_mode = 0# 0: 训练模式; 1: 预测模式\n",
    "predict_threshold = 0.8# 预测时判定是否nomatch的阈值，缺省值为0.8\n",
    "# w2v_dims = # 若不定义，则缺省值为200\n",
    "# model_name = # 如果要载入原有的词向量模型，则再次输入模型名称\n",
    "# 预设文件夹路径\n",
    "pathDir = sys.path[0]+\"\\\\\"# .py文件所在路径\n",
    "mkdir(pathDir+'NN_model\\\\')# 创建神经网络模型存放路径\n",
    "NN_model_path = pathDir+'NN_model\\\\'\n",
    "print('Loading self-defined dict...')# 加载自定义词典\n",
    "try:\n",
    "    jieba.load_userdict(pathDir+\"newdic.txt\")\n",
    "#     print('成功加载自定义分词词库')\n",
    "except Exception as e:\n",
    "    print('Warning! Fail to load: %s.\\nUse default dict and go on...' %e)\n",
    "\n",
    "print('Loading stopwords...')# 获取停用词\n",
    "try:\n",
    "    stopwords = [line.strip() for line in open(pathDir+\"stopwords.txt\",encoding='gb18030',errors='ignore').readlines()]\n",
    "except Exception as e:\n",
    "    print('Warning! fail to load: %s. With no stopwords and go on...' %e)\n",
    "\n",
    "# 默认构建新的词向量模型。若要在原有模型基础上继续训练，build_word2vec函数需传入词向量模型\n",
    "# 若使用原有词向量模型，默认不更新模型\n",
    "w2v_model = build_word2vec(filename=pathDir+'语音转文本_全业务数据集.xlsx', w2v_model='w2v_model', update=False)\n",
    "w2v_model.save(pathDir+date+'_w2v_model')# 保存模型\n",
    "print('词向量模型保存成功：w2v_model')\n",
    "\n",
    "# 将词向量模型加载为数组\n",
    "wd2idx, embedMatrix = build_wd2idx_embedMatrix(w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train_data...\n",
      "模型的层级数为: 3 \n",
      "构造模型树...\n",
      "____________________________________________________________________________________________________\n",
      "根节点下的子节点: [0.0, 1.0]\n",
      "正在训练该节点模型...\n",
      "data_1_len:  26480 label_1_len:  26480 \n",
      "data_2_len:  2943 lebal_2_len:  2943\n",
      "data_1_len:  20601 label_1_len:  20601 \n",
      "data_2_len:  5879 lebal_2_len:  5879\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x_seq (InputLayer)              (None, 38)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 38, 200)      2635200     x_seq[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 38, 32)       29824       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 38, 10)       330         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 37, 10)       650         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 36, 10)       970         lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 35, 10)       1290        lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 34, 10)       1610        lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 1, 10)        12170       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 38, 10)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 37, 10)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 36, 10)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 35, 10)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 34, 10)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 1, 10)        0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 10)           0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 10)           0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 10)           0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 10)           0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 10)           0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 10)           0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 60)           0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 60)           0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            122         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,682,166\n",
      "Trainable params: 46,966\n",
      "Non-trainable params: 2,635,200\n",
      "__________________________________________________________________________________________________\n",
      "This time is:  2019-07-08 17:15:36\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 28s 1ms/step - loss: 0.1416 - acc: 0.9554\n",
      "0.10518358104536009\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.1018 - acc: 0.9682\n",
      "0.09545263276643345\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0915 - acc: 0.9723\n",
      "0.08773551916629532\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0851 - acc: 0.9746\n",
      "0.08833188992696192\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0737 - acc: 0.9774\n",
      "0.08935691251960937\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0736 - acc: 0.9787\n",
      "0.09184474517780221\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0680 - acc: 0.9807\n",
      "0.10533892327613908\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0665 - acc: 0.9818\n",
      "0.1036868272080253\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 25s 1ms/step - loss: 0.0682 - acc: 0.9790\n",
      "0.08831290475140613\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0617 - acc: 0.9816\n",
      "0.08720892594163307\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - ETA: 0s - loss: 0.0565 - acc: 0.983 - 22s 1ms/step - loss: 0.0564 - acc: 0.9832\n",
      "0.09073454912370903\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0503 - acc: 0.9852\n",
      "0.08903157821281443\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0498 - acc: 0.9858\n",
      "0.1096180382074242\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0459 - acc: 0.9872\n",
      "0.10393313718266656\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0441 - acc: 0.9884\n",
      "0.09115192915002505\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 25s 1ms/step - loss: 0.0504 - acc: 0.9843\n",
      "0.08372992415688717\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0420 - acc: 0.9874\n",
      "0.08772979820678166\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0382 - acc: 0.9890\n",
      "0.09736876992772692\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0358 - acc: 0.9895\n",
      "0.09952506430844567\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0339 - acc: 0.9907\n",
      "0.10015836585663712\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 22s 1ms/step - loss: 0.0324 - acc: 0.9911\n",
      "0.10970266120469706\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 27s 1ms/step - loss: 0.0373 - acc: 0.9889\n",
      "0.08811341728534103\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 23s 1ms/step - loss: 0.0345 - acc: 0.9900\n",
      "0.09532116021552116\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 23s 1ms/step - loss: 0.0317 - acc: 0.9903\n",
      "0.0958112206172693\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 24s 1ms/step - loss: 0.0298 - acc: 0.9909\n",
      "0.09994528209426666\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 24s 1ms/step - loss: 0.0276 - acc: 0.9926\n",
      "0.1016347584660528\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 27s 1ms/step - loss: 0.0379 - acc: 0.9895\n",
      "0.08594752198754171\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 25s 1ms/step - loss: 0.0352 - acc: 0.9894\n",
      "0.08653244370075004\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 23s 1ms/step - loss: 0.0337 - acc: 0.9909\n",
      "0.08917584028151969\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 24s 1ms/step - loss: 0.0322 - acc: 0.9911\n",
      "0.09024814994720286\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 24s 1ms/step - loss: 0.0297 - acc: 0.9908\n",
      "0.09328546203423521\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 30s 1ms/step - loss: 0.0386 - acc: 0.9887\n",
      "0.0866728949965292\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 24s 1ms/step - loss: 0.0367 - acc: 0.9896\n",
      "0.08901976829858618\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 24s 1ms/step - loss: 0.0345 - acc: 0.9901\n",
      "0.08677752269605998\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 24s 1ms/step - loss: 0.0335 - acc: 0.9907\n",
      "0.08819633793127585\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 24s 1ms/step - loss: 0.0331 - acc: 0.9903\n",
      "0.0873549461828034\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 30s 1ms/step - loss: 0.0399 - acc: 0.9882\n",
      "0.08619786439336281\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 25s 1ms/step - loss: 0.0387 - acc: 0.9891\n",
      "0.08737651253295692\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 24s 1ms/step - loss: 0.0360 - acc: 0.9899\n",
      "0.08856891758228559\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 24s 1ms/step - loss: 0.0338 - acc: 0.9906\n",
      "0.08997800397188879\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 24s 1ms/step - loss: 0.0339 - acc: 0.9905\n",
      "0.09003248373935752\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 31s 2ms/step - loss: 0.0387 - acc: 0.9887\n",
      "0.08536480173514159\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 24s 1ms/step - loss: 0.0377 - acc: 0.9895\n",
      "0.0864585237248446\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 24s 1ms/step - loss: 0.0380 - acc: 0.9884\n",
      "0.08707721110096633\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 24s 1ms/step - loss: 0.0362 - acc: 0.9894\n",
      "0.08788980218330908\n",
      "Epoch 1/1\n",
      "20601/20601 [==============================] - 24s 1ms/step - loss: 0.0360 - acc: 0.9893\n",
      "0.08782188637821986\n",
      "decay the learning rate\n",
      "processing time: 1209.7892801761627 seconds\n",
      "训练完成！模型名称: 第0层级的模型\n",
      "遍历第1层级0节点下的子节点...\n",
      "____________________________________________________________________________________________________\n",
      "第1层级0节点下的子节点: [0.0, 1.0, 2.0]\n",
      "正在训练该节点模型...\n",
      "data_1_len:  22229 label_1_len:  22229 \n",
      "data_2_len:  2470 lebal_2_len:  2470\n",
      "data_1_len:  17294 label_1_len:  17294 \n",
      "data_2_len:  4935 lebal_2_len:  4935\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x_seq (InputLayer)              (None, 38)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 38, 200)      2635200     x_seq[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 38, 32)       29824       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 38, 10)       330         lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 37, 10)       650         lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 36, 10)       970         lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 35, 10)       1290        lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 34, 10)       1610        lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 1, 10)        12170       lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 38, 10)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 37, 10)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 36, 10)       0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 35, 10)       0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 34, 10)       0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 1, 10)        0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 10)           0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 10)           0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 10)           0           leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 10)           0           leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 10)           0           leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 10)           0           leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 60)           0           global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 60)           0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            183         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,682,227\n",
      "Trainable params: 47,027\n",
      "Non-trainable params: 2,635,200\n",
      "__________________________________________________________________________________________________\n",
      "This time is:  2019-07-08 17:35:52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 27s 2ms/step - loss: 0.3149 - acc: 0.8925\n",
      "0.2127054623445036\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 20s 1ms/step - loss: 0.2327 - acc: 0.9301\n",
      "0.19247560626280452\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 20s 1ms/step - loss: 0.2123 - acc: 0.9386\n",
      "0.20447899147807827\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 20s 1ms/step - loss: 0.1944 - acc: 0.9444\n",
      "0.18355830401664802\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 20s 1ms/step - loss: 0.1864 - acc: 0.9467\n",
      "0.19820988345363363\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 20s 1ms/step - loss: 0.1848 - acc: 0.9486\n",
      "0.19237883804056807\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 20s 1ms/step - loss: 0.1710 - acc: 0.9535\n",
      "0.21875025072862744\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 20s 1ms/step - loss: 0.1594 - acc: 0.9561\n",
      "0.2253907934253515\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 22s 1ms/step - loss: 0.1605 - acc: 0.9576\n",
      "0.20460759172072776\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 28s 2ms/step - loss: 0.1529 - acc: 0.9554\n",
      "0.19719235033309654\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 21s 1ms/step - loss: 0.1399 - acc: 0.9591\n",
      "0.18148255286067122\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 22s 1ms/step - loss: 0.1312 - acc: 0.9607\n",
      "0.18909399054854023\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 24s 1ms/step - loss: 0.1226 - acc: 0.9665\n",
      "0.18942836479619446\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 22s 1ms/step - loss: 0.1226 - acc: 0.9649\n",
      "0.18674692432528084\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 22s 1ms/step - loss: 0.1189 - acc: 0.9659\n",
      "0.180868671891781\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 21s 1ms/step - loss: 0.1110 - acc: 0.9695\n",
      "0.18835669434082652\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 20s 1ms/step - loss: 0.1092 - acc: 0.9709\n",
      "0.19671837613288207\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 22s 1ms/step - loss: 0.1081 - acc: 0.9700\n",
      "0.20275471767915887\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 20s 1ms/step - loss: 0.1074 - acc: 0.9727\n",
      "0.20652779086911485\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 22s 1ms/step - loss: 0.1043 - acc: 0.9724\n",
      "0.1987111487308977\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 28s 2ms/step - loss: 0.0970 - acc: 0.9729\n",
      "0.18686378911921853\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 21s 1ms/step - loss: 0.0866 - acc: 0.9758\n",
      "0.20361999539349243\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 21s 1ms/step - loss: 0.0839 - acc: 0.9755\n",
      "0.2232561855784312\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 21s 1ms/step - loss: 0.0830 - acc: 0.9766\n",
      "0.19730195586196325\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 21s 1ms/step - loss: 0.0751 - acc: 0.9784\n",
      "0.2094456519856144\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 29s 2ms/step - loss: 0.0985 - acc: 0.9728\n",
      "0.18130654275779298\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 22s 1ms/step - loss: 0.0853 - acc: 0.9765\n",
      "0.18265628837320486\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 21s 1ms/step - loss: 0.0796 - acc: 0.9783\n",
      "0.18506426463423953\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 21s 1ms/step - loss: 0.0738 - acc: 0.9797\n",
      "0.18135697634717232\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 23s 1ms/step - loss: 0.0731 - acc: 0.9795\n",
      "0.19049690866820243\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 33s 2ms/step - loss: 0.0947 - acc: 0.9735\n",
      "0.1864941604224294\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 22s 1ms/step - loss: 0.0822 - acc: 0.9788\n",
      "0.18673274465780026\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 23s 1ms/step - loss: 0.0810 - acc: 0.9780\n",
      "0.18811487634534296\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 22s 1ms/step - loss: 0.0776 - acc: 0.9781\n",
      "0.18973443834827497\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 21s 1ms/step - loss: 0.0731 - acc: 0.9788\n",
      "0.1914294336790498\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 34s 2ms/step - loss: 0.0984 - acc: 0.9733\n",
      "0.18305002099227327\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 23s 1ms/step - loss: 0.0890 - acc: 0.9765\n",
      "0.18421755531116535\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 23s 1ms/step - loss: 0.0867 - acc: 0.9768\n",
      "0.18324281809482015\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 23s 1ms/step - loss: 0.0838 - acc: 0.9777\n",
      "0.1818921425142269\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 22s 1ms/step - loss: 0.0807 - acc: 0.9781\n",
      "0.1813880440797883\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 35s 2ms/step - loss: 0.0969 - acc: 0.9746\n",
      "0.18011668596552452\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 29s 2ms/step - loss: 0.0923 - acc: 0.9750\n",
      "0.18223618740132946\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 28s 2ms/step - loss: 0.0903 - acc: 0.9744\n",
      "0.18236429442760915\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 26s 2ms/step - loss: 0.0860 - acc: 0.9769\n",
      "0.18376487188554003\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 27s 2ms/step - loss: 0.0871 - acc: 0.9762: 2s - lo\n",
      "0.18222432556422616\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 25s 1ms/step - loss: 0.0830 - acc: 0.9778\n",
      "0.18419971005154043\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 39s 2ms/step - loss: 0.0937 - acc: 0.9747\n",
      "0.18123275986024243\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 28s 2ms/step - loss: 0.0916 - acc: 0.9756\n",
      "0.1814791173587444\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 28s 2ms/step - loss: 0.0921 - acc: 0.9744\n",
      "0.1821654685897383\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 31s 2ms/step - loss: 0.0876 - acc: 0.9765\n",
      "0.18267896516241042\n",
      "Epoch 1/1\n",
      "17294/17294 [==============================] - 32s 2ms/step - loss: 0.0877 - acc: 0.9768\n",
      "0.18295027894469407\n",
      "decay the learning rate\n",
      "processing time: 1436.749295949936 seconds\n",
      "训练完成！模型名称: 第1层级第0节点的模型\n",
      "遍历第2层级0节点下的子节点...\n",
      "____________________________________________________________________________________________________\n",
      "第2层级0节点下的子节点: [0.0, 1.0]\n",
      "正在训练该节点模型...\n",
      "data_1_len:  11873 label_1_len:  11873 \n",
      "data_2_len:  1320 lebal_2_len:  1320\n",
      "data_1_len:  9237 label_1_len:  9237 \n",
      "data_2_len:  2636 lebal_2_len:  2636\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "x_seq (InputLayer)              (None, 38)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 38, 200)      2635200     x_seq[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 38, 32)       29824       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 38, 10)       330         lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 37, 10)       650         lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 36, 10)       970         lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 35, 10)       1290        lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 34, 10)       1610        lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1, 10)        12170       lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 38, 10)       0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 37, 10)       0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 36, 10)       0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 35, 10)       0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 34, 10)       0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 1, 10)        0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 10)           0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 10)           0           leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 10)           0           leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 10)           0           leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 10)           0           leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 10)           0           leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 60)           0           global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "                                                                 global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "                                                                 global_max_pooling1d_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 60)           0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            122         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,682,166\n",
      "Trainable params: 46,966\n",
      "Non-trainable params: 2,635,200\n",
      "__________________________________________________________________________________________________\n",
      "This time is:  2019-07-08 17:59:59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 24s 3ms/step - loss: 0.1205 - acc: 0.9605\n",
      "0.13635082960015896\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 20s 2ms/step - loss: 0.0807 - acc: 0.9749\n",
      "0.06839465685753209\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 18s 2ms/step - loss: 0.0640 - acc: 0.9802\n",
      "0.0788105035482934\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 17s 2ms/step - loss: 0.0580 - acc: 0.9846:\n",
      "0.07789736537432129\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 17s 2ms/step - loss: 0.0541 - acc: 0.9850\n",
      "0.10558404216373508\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 16s 2ms/step - loss: 0.0469 - acc: 0.9894\n",
      "0.09890195869022246\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 16s 2ms/step - loss: 0.0421 - acc: 0.9899\n",
      "0.0955616762466503\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 31s 3ms/step - loss: 0.0546 - acc: 0.9833\n",
      "0.06938882461664352\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 17s 2ms/step - loss: 0.0469 - acc: 0.9864\n",
      "0.06752524104420886\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 21s 2ms/step - loss: 0.0393 - acc: 0.9899\n",
      "0.06637089968179212\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 20s 2ms/step - loss: 0.0349 - acc: 0.9908\n",
      "0.06652920938689601\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 18s 2ms/step - loss: 0.0330 - acc: 0.9924\n",
      "0.07461307300536922\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 17s 2ms/step - loss: 0.0303 - acc: 0.9927: 0s - loss: 0.0293 \n",
      "0.07389026979605356\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 17s 2ms/step - loss: 0.0270 - acc: 0.9937: 3s - loss: 0.0270 - ETA: 0s - loss: 0.02\n",
      "0.07319820553741672\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 17s 2ms/step - loss: 0.0245 - acc: 0.9945\n",
      "0.07912806375234416\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 29s 3ms/step - loss: 0.0300 - acc: 0.9921\n",
      "0.0784832027317448\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 20s 2ms/step - loss: 0.0239 - acc: 0.9940\n",
      "0.06940951547726537\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 18s 2ms/step - loss: 0.0218 - acc: 0.9939\n",
      "0.08287422944876281\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 17s 2ms/step - loss: 0.0199 - acc: 0.9946\n",
      "0.0684457176195627\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 19s 2ms/step - loss: 0.0160 - acc: 0.9959\n",
      "0.07216225469372038\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 31s 3ms/step - loss: 0.0290 - acc: 0.9918\n",
      "0.07513512902413354\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 20s 2ms/step - loss: 0.0207 - acc: 0.9952\n",
      "0.08237642051764961\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 19s 2ms/step - loss: 0.0201 - acc: 0.9947\n",
      "0.0658920111219314\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 21s 2ms/step - loss: 0.0176 - acc: 0.9957\n",
      "0.07664315064409466\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 21s 2ms/step - loss: 0.0149 - acc: 0.9965\n",
      "0.08336249802148703\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 19s 2ms/step - loss: 0.0142 - acc: 0.9971\n",
      "0.07355509349568323\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 18s 2ms/step - loss: 0.0149 - acc: 0.9963\n",
      "0.07093122280343915\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 20s 2ms/step - loss: 0.0143 - acc: 0.9959\n",
      "0.07730351670108962\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 31s 3ms/step - loss: 0.0156 - acc: 0.9964: 1s - loss: 0.0154 -\n",
      "0.0723368355552807\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 20s 2ms/step - loss: 0.0151 - acc: 0.9965\n",
      "0.07038868484228397\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 20s 2ms/step - loss: 0.0142 - acc: 0.9973\n",
      "0.07753565976621979\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 19s 2ms/step - loss: 0.0132 - acc: 0.9973\n",
      "0.07054725420294386\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 19s 2ms/step - loss: 0.0127 - acc: 0.9974\n",
      "0.08061456715293003\n",
      "decay the learning rate\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 33s 4ms/step - loss: 0.0171 - acc: 0.9958\n",
      "0.06964404968488397\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 22s 2ms/step - loss: 0.0149 - acc: 0.9969\n",
      "0.07079917434038538\n",
      "Epoch 1/1\n",
      "9237/9237 [==============================] - 20s 2ms/step - loss: 0.0143 - acc: 0.9968\n",
      "0.07193749841076857\n",
      "Epoch 1/1\n",
      "2144/9237 [=====>........................] - ETA: 16s - loss: 0.0149 - acc: 0.9953"
     ]
    }
   ],
   "source": [
    "print('Loading train_data...')\n",
    "try:\n",
    "    readbook = xlrd.open_workbook(pathDir+'催收-用户输入-样本及标签-本地全局-类目.xlsx')\n",
    "except Exception as e:\n",
    "    print('Erroe! Fail to load train_data: %s' %e)\n",
    "    sys.exit()\n",
    "sheet1 = readbook.sheet_by_index(0)# sheet1: 文本+标签\n",
    "num_label_lavel = sheet1.ncols-1# 数据集构成为：一列为文本，其余为层级标签。\n",
    "# 动态生成层级标签变量\n",
    "# createVar = locals()\n",
    "# print('Loading label(s)...')\n",
    "# for num, label in enumerate(num_label_lavel):\n",
    "#     try:\n",
    "# #         createVar['model_name_'+str(i)] = name\n",
    "#         createVar['label_'+label] = load_model(NN_model_path+name)\n",
    "#         model_name_lst.append({'name': name, 'model': createVar['model_'+str(i)]})\n",
    "#     except Exception as e:\n",
    "#         print('Warning! Fail to load model(%s): %s. Skip this model and go on to load next model...' %(name, e))\n",
    "# print(model_name_lst)\n",
    "# num_label_lavel = 2\n",
    "print('模型的层级数为:', num_label_lavel, '\\n构造模型树...')\n",
    "print('_'*100)\n",
    "count_lavel = 0# 为方便迭代运算，在层级结构前加入第0层（根节点）\n",
    "find_son(count_lavel, label_index=None)# 迭代训练结构中有多个子节点的节点\n",
    "print('模型训练完毕！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading w2v_model...\n",
      "Loading NN_model...\n",
      "[{'name': '第2层级第0节点的模型', 'model': <keras.engine.training.Model object at 0x000001D800AA0CF8>}]\n"
     ]
    }
   ],
   "source": [
    "# 加载词向量模型\n",
    "print('Loading w2v_model...')\n",
    "try:\n",
    "    w2v_model = Word2Vec.load(pathDir+'w2v_model')\n",
    "except Exception as e:\n",
    "    print('Error! Fail to load model: %s' %e)\n",
    "    sys.exit()\n",
    "# 加载神经网络模型\n",
    "NN_model_path = pathDir+'NN_model\\\\'\n",
    "listTemp = os.listdir(NN_model_path)# 获取模型名称\n",
    "# num_model = len(listTemp)# 获取模型数量\n",
    "# 动态生成变量，加载模型并创建索引\n",
    "createVar = locals()\n",
    "model_name_lst = []\n",
    "print('Loading NN_model...')\n",
    "for num, name in enumerate(listTemp):\n",
    "    try:\n",
    "#         createVar['model_name_'+str(i)] = name\n",
    "        createVar['model_'+str(num)] = load_model(NN_model_path+name)\n",
    "        model_name_lst.append({'name': name, 'model': createVar['model_'+str(num)]})\n",
    "    except Exception as e:\n",
    "        print('Warning! Fail to load model(%s): %s. Skip this model and go on to load next model...' %(name, e))\n",
    "print(model_name_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载自定义分词词库\n"
     ]
    }
   ],
   "source": [
    "# 预测新文本\n",
    "# bug1: 如果输出文本长度超过训练集最长长度，会报错\n",
    "# bug2: 出现新词，会报错\n",
    "# 加载自定义词典\n",
    "num_label_lavel = 3\n",
    "count_lavel = 0# 为方便迭代运算，在层级结构前加入第0层（根节点）\n",
    "jieba.load_userdict(\"newdic.txt\")\n",
    "print('成功加载自定义分词词库')\n",
    "\n",
    "# 获取停用词\n",
    "stopwords = [line.strip() for line in open(\"stopwords.txt\",encoding='gb18030',errors='ignore').readlines()]\n",
    "\n",
    "# 将词向量模型加载为数组\n",
    "wd2idx, embedMatrix = build_wd2idx_embedMatrix(w2v_model)\n",
    "readbook = xlrd.open_workbook(pathDir+'催收-延期还款.xlsx')\n",
    "sheet2 = readbook.sheet_by_index(1)\n",
    "\n",
    "predict_threshold = 0.99\n",
    "fobj_output = open(date+'-predict.txt','a')\n",
    "\n",
    "for i in range(sheet2.nrows-1):\n",
    "    predict_set = sheet2.cell(i+1,0).value\n",
    "# predict_set = input('文本: ')\n",
    "    output = predict_data(count_lavel, predict_set, threshold=predict_threshold)\n",
    "#     print(predict_set, output, 'i:', i)\n",
    "    fobj_output.write('\\n'+str(output))\n",
    "fobj_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "print(model_1.layers[1].output_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '催收文本-newdic.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-37608b1eab73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# num_label_lavel = 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcount_lavel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;31m# 为方便迭代运算，在层级结构前加入第0层（根节点）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_userdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"催收文本-newdic.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'成功加载自定义分词词库'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36mload_userdict\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[0mf_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[0mf_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresolve_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '催收文本-newdic.txt'"
     ]
    }
   ],
   "source": [
    "# 预测新文本\n",
    "# bug1: 如果输出文本长度超过训练集最长长度，会报错\n",
    "# bug2: 出现新词，会报错\n",
    "# 加载自定义词典\n",
    "# num_label_lavel = 3\n",
    "count_lavel = 0# 为方便迭代运算，在层级结构前加入第0层（根节点）\n",
    "jieba.load_userdict(\"催收文本-newdic.txt\")\n",
    "print('成功加载自定义分词词库')\n",
    "\n",
    "# 获取停用词\n",
    "stopwords = [line.strip() for line in open(\"催收-停用词.txt\",encoding='gb18030',errors='ignore').readlines()]\n",
    "\n",
    "# 将词向量模型加载为数组\n",
    "wd2idx, embedMatrix = build_wd2idx_embedMatrix(w2v_model)\n",
    "readbook = xlrd.open_workbook(pathDir+'催收-承诺还款.xlsx')\n",
    "sheet2 = readbook.sheet_by_index(1)\n",
    "\n",
    "predict_threshold = 0.98\n",
    "fobj_output = open(date+'-predict.txt','a')\n",
    "\n",
    "for i in range(sheet2.nrows-1):\n",
    "    predict_set = sheet2.cell(i+1,0).value\n",
    "# predict_set = input('文本: ')\n",
    "    output = predict_data(count_lavel, predict_set, threshold=predict_threshold)\n",
    "#     print(predict_set, output, 'i:', i)\n",
    "    fobj_output.write('\\n'+str(output))\n",
    "fobj_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20190617_第1层级第1节点的模型']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list的模糊匹配\n",
    "import difflib\n",
    "\n",
    "AA = ['20190617_第0层级的模型', '20190617_第1层级第0节点的模型', '20190617_第1层级第1节点的模型']\n",
    "A = '20190617_第1层级第4节点的模型'\n",
    "a = difflib.get_close_matches(A,AA,1, cutoff=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load('20190617_w2v_model')\n",
    "model_1 = load_model('20190610_一级模型_1')\n",
    "model_2 = load_model('20190610_一级模型_2')\n",
    "model_3 = load_model('20190610_一级模型_3')\n",
    "lmodel_1 = load_mo0del('20190610_二级（本地）模型_1')\n",
    "lmodel_2 = load_model('20190610_二级（本地）模型_2')\n",
    "lmodel_3 = load_model('20190610_二级（本地）模型_3')\n",
    "gmodel_1 = load_model('20190610_二级（全局）模型_1')\n",
    "gmodel_2 = load_model('20190610_二级（全局）模型_2')\n",
    "gmodel_3 = load_model('20190610_二级（全局）模型_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "微信 ['微信'] label:  7\n",
      "这个我不懂了 ['这个', '我', '不', '懂了'] label:  6\n",
      "为什么不提前跟我说啊我一下子哪有那么多钱 ['为什么', '不', '提前', '跟', '我', '说', '啊', '我', '一下子', '哪有', '那么', '多', '钱'] label:  15\n",
      "你这是多少利息了 ['你', '这是', '多少', '利息', '了'] label:  17\n",
      "处理中遇到问题 ['处理', '中', '遇到', '问题'] label:  7\n",
      "我的到来人工聊妈吗他那卡里的吗我怎么登不了啊 ['我的', '到来', '人工', '聊妈', '吗', '他', '那', '卡里', '吗', '我', '怎么', '登', '不了', '啊'] label:  7\n",
      "喂喂你听的见吧 ['喂', '喂', '你', '听的见', '吧'] label:  0\n",
      "呃他妈的这什么意思哦 ['呃', '他妈的', '这', '什么', '意思', '哦'] label:  3\n",
      "呃我想请问你一下你们可以听我说一下吗因为我的那个卡绑定的那个卡换掉了我的那个卡掉了现在我要重新绑定一张卡然后我把 ['呃', '我', '想', '请问', '你', '一下', '你们', '可以', '听', '我', '说', '一下', '吗', '因为', '我的', '那个', '卡', '绑定', '那个', '卡换', '掉', '了', '我的', '那个', '卡', '掉', '了', '现在', '我', '要', '重新', '绑定', '一张', '卡', '我', '把'] label:  5\n",
      "喂你给我再说啊 ['喂', '你', '给我', '再说', '啊'] label:  6\n",
      "我不知道为什么 ['我', '不知道', '为什么'] label:  7\n",
      "唉你等一下我因为我刚好今天从工作的基础里好吧你你从讲好吧 ['唉', '你', '等一下', '我', '因为', '我', '刚好', '今天', '从', '工作', '基础', '里', '好吧', '你', '你', '从', '讲', '好吧'] label:  6\n",
      "你他妈谁啊 ['你', '他', '妈', '谁', '啊'] label:  3\n",
      "呃我说我我我想问一下这个时间更改吗因为我这个发工资的日期是十号发工资还没发工资了关键是要我想问一下咱们那个 ['呃', '我', '说', '我', '我', '我', '想', '问一下', '这个', '时间', '更改', '吗', '因为', '我', '这个', '发工资', '日期', '是', '十号', '发工资', '还没', '发工资', '了', '关键', '是', '要', '我', '想', '问一下', '咱们', '那个'] label:  2\n",
      "别的银行能扣么 ['别的', '银行', '能扣', '么'] label:  7\n",
      "喂你帮我你们怎么没有那个提前还款呢 ['喂', '你', '帮', '我', '你们', '怎么', '没有', '那个', '提前', '还款', '呢'] label:  7\n",
      "你们能能不能不要用那个语音机器人啊 ['你们', '能', '能不能', '不要', '用', '那个', '语音', '机器人', '啊'] label:  4\n",
      "你好能听到我说话吗喂你好 ['你好', '能', '听到', '我', '说话', '吗', '喂', '你好'] label:  6\n",
      "听得到听得到 ['听得到', '听得到'] label:  6\n",
      "呃对什么问题 ['呃', '对', '什么', '问题'] label:  3\n",
      "没错是我 ['没错', '是', '我'] label:  6\n",
      "你会不会说话啊喂喂 ['你', '会不会', '说话', '啊', '喂', '喂'] label:  6\n",
      "不是没钱是不是给忘记处理了那个银行卡是不是要能改银行卡嘛就是啊 ['不是', '没钱', '是不是', '给', '忘记处理', '了', '那个', '银行卡', '是不是', '要', '能', '改', '银行卡', '嘛', '啊'] label:  7\n",
      "什么鬼 ['什么', '鬼'] label:  6\n",
      "不是我我前几天把银行卡丢了然后现在全人权不了我银行卡号记不得 ['不是', '我', '我', '前几天', '把', '银行卡', '丢了', '现在', '全', '人权', '不了', '我', '银行卡号', '记不得'] label:  10\n",
      "怎么办啊现在是不是最后一期了 ['怎么办', '啊', '现在', '是不是', '最后', '一期', '了'] label:  13\n",
      "手续费 ['手续费'] label:  17\n",
      "人工智能 ['人工智能'] label:  4\n",
      "就是账户专用账户中自动扣款的吗 ['账户', '专用', '账户', '中', '自动扣款', '吗'] label:  7\n",
      "然后是几天说一下 ['是', '几天', '说', '一下'] label:  12\n",
      "是客服打过来的吗 ['是', '客服', '打', '过来', '吗'] label:  4\n",
      "在哪啊 ['在', '哪', '啊'] label:  7\n",
      "我要人跟我说话 ['我', '要', '人', '跟', '我', '说话'] label:  5\n",
      "是客服是吧 ['是', '客服', '是', '吧'] label:  3\n",
      "喂他怎么了 ['喂', '他', '怎么', '了'] label:  3\n",
      "喂你好我确认客服人员还是那个系统自己打过来的 ['喂', '你好', '我', '确认', '客服', '人员', '还是', '那个', '系统', '自己', '打', '过来'] label:  4\n",
      "我现在在山里面信号不好喂 ['我', '现在', '在', '山', '里面', '信号不好', '喂'] label:  6\n",
      "需要您打电话干嘛 ['需要', '您', '打电话', '干嘛'] label:  3\n",
      "呃想问点什么的 ['呃', '想', '问点', '什么'] label:  0\n",
      "是我听的见吧 ['是', '我', '听的见', '吧'] label:  0\n",
      "已经逾期几天了啊 ['已经', '逾期', '几天', '了', '啊'] label:  12\n",
      "哎呀我也不知道这这个怎么回事 ['哎呀', '我', '也', '不知道', '这', '这个', '怎么回事'] label:  7\n",
      "Accuracy: 95.394737\n",
      "Loss: 0.234351\n",
      "This time is:  2019-06-14 15:35:51\n"
     ]
    }
   ],
   "source": [
    "# 测试模型，传入参数：用于测试的模型，字典，测试数据，测试标签\n",
    "test_model(model_1, g_dictionary, gtstd, gtstl, detail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "readbook = xlrd.open_workbook('催收-用户输入-样本及标签-本地全局-类目.xlsx')\n",
    "sheet2 = readbook.sheet_by_name('pos_stopwords')\n",
    "sheet3 = readbook.sheet_by_name('predict')\n",
    "pos_stopwords = []\n",
    "for i in range(sheet2.nrows):\n",
    "    pos_stopwords.append(sheet2.cell(i,0).value)\n",
    "predict_set = []\n",
    "for i in range(sheet3.nrows-1):\n",
    "    predict_set.append(sheet3.cell(i+1,0).value)\n",
    "wd2idx, embedMatrix = build_wd2idx_embedMatrix(w2v_model)\n",
    "# 加载自定义词典\n",
    "jieba.load_userdict(\"催收文本-newdic.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\admin\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-0dc9e9d25bd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mwd2idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedMatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_wd2idx_embedMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# 加载自定义词典\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_userdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"催收文本-newdic.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36mload_userdict\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[0mWord\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mignored\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         '''\n\u001b[1;32m--> 371\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[0mf_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36mcheck_initialized\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcalc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDAG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroute\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36minitialize\u001b[1;34m(self, dictionary)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFREQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarshal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m                     \u001b[0mload_from_cache_fail\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "readbook = xlrd.open_workbook('催收-用户输入-样本及标签-本地全局-类目.xlsx')\n",
    "sheet2 = readbook.sheet_by_name('pos_stopwords')\n",
    "pos_stopwords = []\n",
    "for i in range(sheet2.nrows):\n",
    "    pos_stopwords.append(sheet2.cell(i,0).value)\n",
    "wd2idx, embedMatrix = build_wd2idx_embedMatrix(w2v_model)\n",
    "# 加载自定义词典\n",
    "jieba.load_userdict(\"催收文本-newdic.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('估计', 0.7111688852310181), ('倒把', 0.6890275478363037), ('不照', 0.6791861057281494), ('活转', 0.6752203702926636), ('甚至', 0.6734603643417358), ('要明', 0.6718773245811462), ('微软', 0.6554404497146606), ('留款', 0.6554221510887146), ('越过', 0.6531094312667847), ('少扣', 0.6469331979751587)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.most_similar(positive=['可能']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\biocloo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.945 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好嘞 ['好', '嘞']\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict(\"催收文本-newdic.txt\")\n",
    "sts1 = '好嘞'\n",
    "sts1_cut = jieba.lcut(sts1)\n",
    "print(sts1,sts1_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 2], [0, 2, 1], [1, 17, 0]]\n"
     ]
    }
   ],
   "source": [
    "tst_label = [[0,0,2], [0,2,1], [1,17,0]]\n",
    "print(tst_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "def load_data_set():\n",
    "    try:\n",
    "        readbook = xlrd.open_workbook(pathDir+'催收-还款意图模糊.xlsx')\n",
    "    except Exception as e:\n",
    "        raise TypeError('Erroe! Fail to load train_data: %s' %e)\n",
    "    sheet1 = readbook.sheet_by_index(0)# sheet1: 文本+标签\n",
    "    num_label_lavel = sheet1.ncols-1# 数据集构成为：一列为文本，其余为层级标签。\n",
    "    data = []\n",
    "    label = []\n",
    "    for i in range(sheet1.nrows-1):\n",
    "        data.append(sheet1.cell(i+1, 0).value)\n",
    "        lb_tmp = []\n",
    "        for layer in range(num_label_lavel):\n",
    "            lb_tmp.append(sheet1.cell(i+1, layer+1).value)\n",
    "        label.append(lb_tmp)\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(np.column_stack([0, 1, 0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
